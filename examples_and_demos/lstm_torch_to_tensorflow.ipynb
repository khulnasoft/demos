{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q startai"
      ],
      "metadata": {
        "id": "fsW9YucKLwmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "489378ea-a054-468b-bf11-7011924398b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF-rwYGRCF9j"
      },
      "outputs": [],
      "source": [
        "import startai\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create torch lstm layer\n",
        "torch_lstm = torch.nn.LSTM(2, 2, 1).to(\"cuda\")\n",
        "\n",
        "# transpile lstm layer to tensorflow\n",
        "x = torch.rand((5, 2, 2)).cuda()\n",
        "tf_lstm = startai.transpile(torch_lstm, source=\"torch\", to=\"tensorflow\", args=(x,))"
      ],
      "metadata": {
        "id": "q2LNaAXxDWc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba1bf4f2-81c2-45ea-baa5-59c9cf5c2a43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/startai/utils/exceptions.py:383: UserWarning: The current backend: 'tensorflow' does not support inplace updates natively. Startai would quietly create new arrays when using inplace updates with this backend, leading to memory overhead (same applies for views). If you want to control your memory management, consider doing startai.set_inplace_mode('strict') which should raise an error whenever an inplace update is attempted with this backend.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get output of original torch lstm layer\n",
        "x = torch.rand((20, 32, 2)).cuda()\n",
        "original_output = torch_lstm(x)\n",
        "\n",
        "# get output of transpiled tf lstm layer with the same input\n",
        "x = tf.constant(x.cpu().numpy())\n",
        "transpiled_output = tf_lstm(x)\n",
        "\n",
        "# verify the outputs are the same (with some tolerance)\n",
        "np.allclose(original_output[0].detach().cpu(), transpiled_output[0].numpy(), atol=1e-7)"
      ],
      "metadata": {
        "id": "yPLdIj2CD7pA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43e3b59-2ca3-49f6-f6d0-0a5e8d14dd73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run some benchmarks\n",
        "from time import perf_counter\n",
        "\n",
        "x = torch.rand((20, 32, 2)).cuda()\n",
        "N_RUNS = 1000\n",
        "\n",
        "# time the original torch lstm\n",
        "s = perf_counter()\n",
        "for _ in range(N_RUNS):\n",
        "  torch_lstm(x)\n",
        "original_torch_time = perf_counter() - s\n",
        "\n",
        "# compile transpiled tf lstm\n",
        "x = tf.constant(x.cpu().numpy())\n",
        "tf_lstm = tf.autograph.experimental.do_not_convert(tf_lstm)\n",
        "compiled_tf_lstm = tf.function(tf_lstm)\n",
        "compiled_tf_lstm(x)\n",
        "\n",
        "# time the transpiled tf lstm\n",
        "s = perf_counter()\n",
        "for _ in range(N_RUNS):\n",
        "  compiled_tf_lstm(x)\n",
        "transpiled_tf_time = perf_counter() - s\n",
        "\n",
        "# time tf's own lstm layer (also compiled) for comparison\n",
        "original_tf_lstm = tf.keras.layers.LSTM(2, time_major=True, return_sequences=True)\n",
        "original_tf_lstm = tf.function(original_tf_lstm)\n",
        "original_tf_lstm(x)\n",
        "\n",
        "s = perf_counter()\n",
        "for _ in range(N_RUNS):\n",
        "  original_tf_lstm(x)\n",
        "original_tf_time = perf_counter() - s\n",
        "\n",
        "# as we can see, the transpiled tf lstm has comparable performance to tf's own lstm layer\n",
        "print(f'transpiled tf time is {transpiled_tf_time / original_torch_time}x slower than torch\\'s lstm')\n",
        "print(f'original tf lstm time is {original_tf_time / original_torch_time}x slower than torch\\'s lstm')"
      ],
      "metadata": {
        "id": "22XU0XD277Y4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91c35ac9-b282-46a2-b33a-963c9dbc1cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transpiled tf time is 4.480074623755541x slower than torch's lstm\n",
            "original tf lstm time is 2.362692848996253x slower than torch's lstm\n"
          ]
        }
      ]
    }
  ]
}