{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "First, lets install Startai via pip and import it alongside the other frameworks we'll be using."
      ],
      "metadata": {
        "id": "nZlFgbLdA9lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q startai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmN-gYwICZjy",
        "outputId": "2e969212-08bf-443d-86d2-fe171d593628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TfqvcChjoE5"
      },
      "outputs": [],
      "source": [
        "import startai\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we create a basic TensorFlow Keras model containing a single LSTM layer as an example.\n",
        "\n",
        "We can then convert this model to PyTorch by transpiling with startai.transpile and providing some input arguments for the model."
      ],
      "metadata": {
        "id": "8pt3dWIjBXM2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UPdMybhjd5G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2105d54f-7e6a-4b14-8f57-bdff4f860f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:can not set PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') to dynamically allocate memory. Physical devices cannot be modified after being initialized\n",
            "/usr/local/lib/python3.10/dist-packages/startai/utils/exceptions.py:383: UserWarning: The current backend: 'tensorflow' does not support inplace updates natively. Startai would quietly create new arrays when using inplace updates with this backend, leading to memory overhead (same applies for views). If you want to control your memory management, consider doing startai.set_inplace_mode('strict') which should raise an error whenever an inplace update is attempted with this backend.\n",
            "  warnings.warn(\n",
            "WARNING:root:can not set PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') to dynamically allocate memory. Physical devices cannot be modified after being initialized\n",
            "/usr/local/lib/python3.10/dist-packages/startai/compiler/compiler.py:164: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return _transpile(\n",
            "/usr/local/lib/python3.10/dist-packages/startai/compiler/compiler.py:164: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return _transpile(\n"
          ]
        }
      ],
      "source": [
        "with tf.device(\"/CPU:0\"):\n",
        "  sample_input = tf.random.uniform((5, 2, 40))\n",
        "\n",
        "  # build the lstm keras model\n",
        "  tf_lstm = tf.keras.Sequential([tf.keras.layers.LSTM(40)])\n",
        "  tf_lstm.build(sample_input.shape)\n",
        "\n",
        "  # transpile to torch\n",
        "  torch_lstm = startai.transpile(tf_lstm, source=\"tensorflow\", to=\"torch\", args=(sample_input,))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've transpiling the model to PyTorch, lets verify that the results produced by the new PyTorch model are identical to those produced by the original Keras model.\n",
        "\n",
        "We'll use an input tensor with different shape to the input the model was transpiled with, to verify that the transpiled model is compatible with dynamic input shapes."
      ],
      "metadata": {
        "id": "cL_5ZJ7tCEJb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LlLeYU6kAdy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d019e2e-e379-4acd-ae46-5ff0951dffd5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# identical input tensors for torch and tf\n",
        "torch_input = torch.rand((10, 100, 40))\n",
        "tf_input = tf.constant(torch_input.cpu().detach().numpy())\n",
        "\n",
        "# compile the original tensorflow model\n",
        "tf_lstm = tf.function(tf_lstm)\n",
        "\n",
        "# get output of the original and transpiled models\n",
        "tf_output = tf_lstm(tf_input)\n",
        "torch_output = torch_lstm(torch_input)\n",
        "\n",
        "# verify the outputs are the same (with some tolerance)\n",
        "np.allclose(tf_output[0].numpy(), torch_output[0].cpu().detach().numpy(), atol=1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, lets benchmark the transpiled torch model compared to the original. Here we benchmark on both CPU and GPU over 1000 inference runs."
      ],
      "metadata": {
        "id": "a8Z8HYfVCR5V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwIxkmwGkIiT",
        "outputId": "dd74861f-d304-4d04-ad1b-20d2b48e14e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(CPU)  tensorflow lstm time: 5.5017 seconds  transpiled torch lstm time: 2.1101 seconds\n",
            "(GPU)  tensorflow lstm time: 1.7519 seconds  transpiled torch lstm time: 0.901 seconds\n",
            "\n",
            "transpiled torch lstm is 2.607x faster than tensorflow's lstm on CPU\n",
            "transpiled torch lstm is 1.944x faster than tensorflow's lstm on GPU\n"
          ]
        }
      ],
      "source": [
        "# run some benchmarks\n",
        "\n",
        "from time import perf_counter\n",
        "\n",
        "N_RUNS = 1000\n",
        "\n",
        "tf_inputs = tf.random.normal([10, 100, 40])\n",
        "torch_inputs = torch.from_numpy(tf_inputs.numpy())\n",
        "\n",
        "\n",
        "# benchmark on CPU\n",
        "with tf.device(\"/CPU:0\"):\n",
        "  torch_lstm = torch_lstm.to(\"cpu\")\n",
        "\n",
        "  tf_inputs = tf.random.normal([10, 100, 40])\n",
        "  torch_inputs = torch.from_numpy(tf_inputs.numpy()).to(\"cpu\")\n",
        "\n",
        "  # time the tf lstm\n",
        "  s = perf_counter()\n",
        "  for _ in range(N_RUNS):\n",
        "    tf_lstm(tf_inputs)\n",
        "  tf_time = round(perf_counter() - s, 4)\n",
        "\n",
        "  # time the transpiled torch lstm\n",
        "  s = perf_counter()\n",
        "  for _ in range(N_RUNS):\n",
        "    torch_lstm(torch_inputs)\n",
        "  torch_time = round(perf_counter() - s, 4)\n",
        "\n",
        "  print(f'(CPU)  tensorflow lstm time: {tf_time} seconds  transpiled torch lstm time: {torch_time} seconds')\n",
        "\n",
        "  cpu_speedup = round(tf_time / torch_time, 3)\n",
        "\n",
        "\n",
        "# benchmark on GPU\n",
        "with tf.device(\"/GPU:0\"):\n",
        "  torch_lstm = torch_lstm.cuda()\n",
        "\n",
        "  tf_inputs = tf.random.normal([10, 100, 40])\n",
        "  torch_inputs = torch.from_numpy(tf_inputs.numpy()).cuda()\n",
        "\n",
        "  # time the original tf lstm\n",
        "  s = perf_counter()\n",
        "  for _ in range(N_RUNS):\n",
        "    tf_lstm(tf_inputs)\n",
        "  tf_time = round(perf_counter() - s, 4)\n",
        "\n",
        "  # time the transpiled torch lstm\n",
        "  s = perf_counter()\n",
        "  for _ in range(N_RUNS):\n",
        "    torch_lstm(torch_inputs)\n",
        "  torch_time = round(perf_counter() - s, 4)\n",
        "\n",
        "  print(f'(GPU)  tensorflow lstm time: {tf_time} seconds  transpiled torch lstm time: {torch_time} seconds')\n",
        "\n",
        "  gpu_speedup = round(tf_time / torch_time, 3)\n",
        "\n",
        "\n",
        "# the transpiled torch lstm is faster than tensorflow's lstm layer on both cpu and gpu\n",
        "print(f'\\ntranspiled torch lstm is {cpu_speedup}x faster than tensorflow\\'s lstm on CPU')\n",
        "print(f'transpiled torch lstm is {gpu_speedup}x faster than tensorflow\\'s lstm on GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the results of the transpiled PyTorch model are significantly faster than the original Keras model on both CPU and GPU :)"
      ],
      "metadata": {
        "id": "8o02WQqpDlyS"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}